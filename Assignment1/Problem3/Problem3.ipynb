{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Problem3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MalihaUCF/Machine-Learning-Course-Assignments-Spring-2019/blob/master/Assignment1/Problem3/Problem3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "db43161b-ed8f-4da7-b4d9-62ebe1606b49",
        "id": "5k5D4xHa5mlG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "cell_type": "code",
      "source": [
        "# We are going to classify all 10 classes of MNIST dataset using Softmax and SGD\n",
        "\n",
        "\n",
        "#loading MNIST dataset\n",
        "from keras.datasets import mnist\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "(train_images,train_labels),(test_images,test_labels)=mnist.load_data()  \n",
        "\n",
        "\n",
        "\n",
        "print('Training Data Shape',train_images.shape)\n",
        "print('Test Data Shape',test_images.shape)\n",
        "print('The train and test labels look like this,' ,train_labels.shape,test_labels.shape)\n",
        "\n",
        "\n",
        "train_images = train_images.reshape(60000, 28*28) \n",
        "test_images = test_images.reshape(10000, 28*28)\n",
        "\n",
        "#Transposing the matrices\n",
        "train_images = train_images.T \n",
        "test_images=test_images.T\n",
        "\n",
        "\n",
        "print(train_images.shape)\n",
        "print(train_labels.shape)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 1s 0us/step\n",
            "Training Data Shape (60000, 28, 28)\n",
            "Test Data Shape (10000, 28, 28)\n",
            "The train and test labels look like this, (60000,) (10000,)\n",
            "(784, 60000)\n",
            "(60000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "2101b8bf-1c57-4229-ab6f-0dfb25e3d911",
        "id": "4EujSyMl5mlL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#Normalizing and shuffling the images\n",
        "train_images = train_images.astype('float32') \n",
        "test_images = test_images.astype('float32') \n",
        "train_images /= 255 \n",
        "test_images /= 255\n",
        "\n",
        "np.random.seed(138)\n",
        "shuffle_index = np.random.permutation(60000)\n",
        "print(shuffle_index)\n",
        "#train_images, train_labels = train_images[:,shuffle_index], train_labels[:,shuffle_index]\n",
        "print(train_images.shape)\n",
        "print(train_labels.shape)\n",
        "\n",
        "#reshaping the train and test labels\n",
        "train_labels, test_labels = train_labels.reshape(1,60000),test_labels.reshape(1,10000)\n",
        "\n",
        "print('Y-reshape-',train_labels.shape)\n",
        "print('Y-reshape-',test_labels.shape)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[45084 33389 21425 ... 33653 57833 58618]\n",
            "(784, 60000)\n",
            "(60000,)\n",
            "Y-reshape- (1, 60000)\n",
            "Y-reshape- (1, 10000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "5f315708-6e73-4cc2-a5fa-0978cd124a0b",
        "id": "ADSfNvVx5mlP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        }
      },
      "cell_type": "code",
      "source": [
        "#Converting the labels to one hot vectors using numpy \n",
        "\n",
        "train_labels_new = np.eye(10)[train_labels.astype('int32')]\n",
        "test_labels_new=np.eye(10)[test_labels.astype('int32')]\n",
        "\n",
        "print(train_labels_new.shape)\n",
        "\n",
        "train_labels_new=train_labels_new.T.reshape(10,60000)\n",
        "test_labels_new=test_labels_new.T.reshape(10,10000)\n",
        "\n",
        "print(train_labels_new.shape)\n",
        "\n",
        "\n",
        "#visualizing an image to see if everything till now is working fine\n",
        "i = 12\n",
        "plt.imshow(train_images[:,i].reshape(28,28), cmap = matplotlib.cm.binary)\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "print('One hot vector',train_labels_new[:,i])\n",
        "print(train_labels[:,i])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 60000, 10)\n",
            "(10, 60000)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAD4CAYAAADFJPs2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIHQIIHQIIHQIIHQI8D+ghgodDEZSyAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "One hot vector [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "[3]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "MCbt-vAI5mlU",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#defining the sigmoid activation\n",
        "\n",
        "def sigmoid(z):\n",
        "    s = 1 / (1 + np.exp(-z))\n",
        "    return s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "A6Xzt_495mlX",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Here we define the categorical cross entropy which is the loss function to be used with Softmax activation and multiclass classification\n",
        "\n",
        "def compute_multiclass_loss(Y, Y_hat):  # categorical cross entropy loss\n",
        "\n",
        "    L_sum = np.sum(np.multiply(Y, np.log(Y_hat)))   # L=-sum(yk.logak) where ak=e^zk/sum(e^zj)\n",
        "    m = Y.shape[1]\n",
        "    L = -(1/m) * L_sum\n",
        "\n",
        "    return L"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "7a3L3pc15mlZ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#this function splits the training set into slices to train in batches\n",
        "\n",
        "def mini_batches(X_whole, Y_whole, batchsize):\n",
        "  for index in range(0, X_whole.shape[0] - batchsize + 1, batchsize):\n",
        "    batch = slice(index, index + batchsize)\n",
        "    yield X_whole[batch], Y_whole[batch]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "65478fc3-5e3d-4eb6-ad3b-516fd58716f3",
        "id": "bo8HcKVk4sqb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#we define the hyperparameters\n",
        "learning_rate = 0.01\n",
        "batch_size=128\n",
        "\n",
        "X = train_images\n",
        "Y = train_labels_new #one hot encoded\n",
        "\n",
        "n_x = X.shape[0]\n",
        "m = X.shape[1]\n",
        "\n",
        "#we initialize the weight and bias\n",
        "W = np.random.randn(n_x, 10) * 0.01\n",
        "b = np.zeros((1, 1))\n",
        "\n",
        "#we perform backward propagation here\n",
        "\n",
        "for i in range(2000):\n",
        "    Z = np.matmul(W.T, X) + b\n",
        "    A = np.exp(Z) / np.sum(np.exp(Z), axis=0) #We define softmax here -> ez/sum(ez)\n",
        "\n",
        "    cost = compute_multiclass_loss(Y, A)  # we call the categorical cross entropy function here\n",
        "\n",
        "    dW = (1/m) * np.matmul(X, (A-Y).T)\n",
        "    db = (1/m) * np.sum(A-Y, axis=1, keepdims=True)\n",
        "\n",
        "    W = W - learning_rate * dW\n",
        "    b = b - learning_rate * db\n",
        "\n",
        "    if (i % 100 == 0):\n",
        "        print(\"Epoch\", i, \"cost: \", cost)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 cost:  2.313683460438797\n",
            "Epoch 100 cost:  1.558805850273726\n",
            "Epoch 200 cost:  1.1956851967922217\n",
            "Epoch 300 cost:  1.0001748018810674\n",
            "Epoch 400 cost:  0.8805394139174488\n",
            "Epoch 500 cost:  0.7998640326293867\n",
            "Epoch 600 cost:  0.741562188078741\n",
            "Epoch 700 cost:  0.6972608813942966\n",
            "Epoch 800 cost:  0.6623109736750714\n",
            "Epoch 900 cost:  0.6339295438820168\n",
            "Epoch 1000 cost:  0.6103485421074648\n",
            "Epoch 1100 cost:  0.5903896264886369\n",
            "Epoch 1200 cost:  0.5732361934252469\n",
            "Epoch 1300 cost:  0.558303707929675\n",
            "Epoch 1400 cost:  0.5451622087567701\n",
            "Epoch 1500 cost:  0.5334880173646069\n",
            "Epoch 1600 cost:  0.5230325514532221\n",
            "Epoch 1700 cost:  0.513601555226432\n",
            "Epoch 1800 cost:  0.5050408918450359\n",
            "Epoch 1900 cost:  0.4972265939660267\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "b4xuiTGR4SeF",
        "outputId": "40e82a70-f8b5-466e-bdeb-02c86e3c03df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "cell_type": "code",
      "source": [
        "#we test our model using sklearn functions, we get 89 % accurcay as can be seen below\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "Z2 = np.matmul(W.T, test_images) + b\n",
        "A2 = np.exp(Z2) / np.sum(np.exp(Z2), axis=0)\n",
        "\n",
        "predictions = np.argmax(A2, axis=0)\n",
        "labels = np.argmax(test_labels_new, axis=0)\n",
        "\n",
        "#confusion matrix gives us true positives-correct predictions in the diagonal of the matrix\n",
        "print(confusion_matrix(predictions, labels))\n",
        "\n",
        "# the classification report shows 89% tets accuracy\n",
        "print(classification_report(predictions, labels))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 951    0   13    6    2   19   20    4    9   15]\n",
            " [   0 1097   13    2    7   11    3   25   10    9]\n",
            " [   2    4  859   20    5    5   11   32   13    9]\n",
            " [   3    4   26  883    0   54    2    1   34   12]\n",
            " [   0    1   20    1  886   22   14   13   12   45]\n",
            " [   2    2    1   38    1  694   19    0   25   15]\n",
            " [  14    4   22    7   14   20  884    2   16    1]\n",
            " [   1    0   24   17    2   11    0  906   13   29]\n",
            " [   7   23   45   22    9   42    5    5  826    8]\n",
            " [   0    0    9   14   56   14    0   40   16  866]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.92      0.94      1039\n",
            "           1       0.97      0.93      0.95      1177\n",
            "           2       0.83      0.89      0.86       960\n",
            "           3       0.87      0.87      0.87      1019\n",
            "           4       0.90      0.87      0.89      1014\n",
            "           5       0.78      0.87      0.82       797\n",
            "           6       0.92      0.90      0.91       984\n",
            "           7       0.88      0.90      0.89      1003\n",
            "           8       0.85      0.83      0.84       992\n",
            "           9       0.86      0.85      0.86      1015\n",
            "\n",
            "   micro avg       0.89      0.89      0.89     10000\n",
            "   macro avg       0.88      0.88      0.88     10000\n",
            "weighted avg       0.89      0.89      0.89     10000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}