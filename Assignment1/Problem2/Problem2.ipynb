{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Problem2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MalihaUCF/Machine-Learning-Course-Assignments--Spring-2019/blob/master/Assignment1/Problem2/Problem2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "-BUSngBkoCT9",
        "colab_type": "code",
        "outputId": "8980b34e-be29-49bb-c5a0-73907ce7d1e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "#This code trains 10 neural network classifiers using Binary Cross Entropy Loss\n",
        "\n",
        "\n",
        "#Code Author : Maliha Arif\n",
        "#PID:4506817\n",
        "    \n",
        "\n",
        "#Downloading MNIST dataset\n",
        "\n",
        "from keras.datasets import mnist\n",
        "import numpy as np\n",
        "\n",
        "(train_images_original,train_labels_original),(test_images_original,test_labels_original)=mnist.load_data()  \n",
        "\n",
        "\n",
        "\n",
        "print('Training Data Shape',train_images_original.shape)\n",
        "print('Test Data Shape',test_images_original.shape)\n",
        "print('The train and test labels look like,' ,train_labels_original.shape,test_labels_original.shape)\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Data Shape (60000, 28, 28)\n",
            "Test Data Shape (10000, 28, 28)\n",
            "The train and test labels look like, (60000,) (10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NXIm2hLaseHw",
        "colab_type": "code",
        "outputId": "f9caafc0-0551-4c69-ca5a-85168f080ec6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "#The downloaded images are reshaped to 1D feature vector of size 28*28\n",
        "\n",
        "\n",
        "train_images_original = train_images_original.reshape(60000, 28*28) \n",
        "test_images_original = test_images_original.reshape(10000, 28*28)\n",
        "\n",
        "train_images_original = train_images_original.T \n",
        "test_images_original=test_images_original.T\n",
        "\n",
        " \n",
        "train_images_original = train_images_original.astype('float32') \n",
        "test_images_original = test_images_original.astype('float32') \n",
        "\n",
        "#The images are normalized to have values between 0 and 1\n",
        "train_images_original /= 255 \n",
        "test_images_original /= 255\n",
        "\n",
        "\n",
        "print(train_images_original.shape)\n",
        "print(test_images_original.shape)\n",
        "print(train_labels_original.shape)\n",
        "print(test_labels_original.shape)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(784, 60000)\n",
            "(784, 10000)\n",
            "(60000,)\n",
            "(10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DVyZq_Sjsepo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Here we modify class labels using np.where so that we can train a 1 class classifier that identifies class 5\n",
        "#Train labels with digit 5 label are labelled as 1 and rest other digits are labelled 0\n",
        "\n",
        "def modify_labels(digit,train_labels_original,test_labels_original):\n",
        "  \n",
        "  y_new = np.zeros(train_labels_original.shape)\n",
        "  y_new[np.where(train_labels_original == digit)[0]] = 1\n",
        "  train_labels = y_new\n",
        "\n",
        "  y_new = np.zeros(test_labels_original.shape)\n",
        "  y_new[np.where(test_labels_original == digit)[0]] = 1\n",
        "  test_labels = y_new\n",
        "\n",
        "  return train_labels,test_labels\n",
        "  #print(train_labels.shape)\n",
        "  #print(test_labels.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Rsnm45ZsuLQu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#Here we define activation and all loss functions\n",
        "\n",
        "#defining sigmoid activation function\n",
        "def sigmoid(z):\n",
        "  s = 1 / (1 + np.exp(-z))\n",
        "  return s\n",
        "\n",
        "\n",
        "#defining mean squared error loss\n",
        "def mse(Y,A):   \n",
        "  \n",
        "  m =  m = Y.shape[1]\n",
        "  L = np.square(Y - A).mean()\n",
        "  return L\n",
        "  \n",
        "  \n",
        "#defining mean squared error loss with activation function\n",
        "def squared_loss(w,b,X,Y):\n",
        "  \n",
        "  m =  m = X.shape[1]\n",
        "  A = sigmoid(np.dot(w.T, X) + b)\n",
        "  L = np.square(Y - A).mean()   #calculating loss\n",
        " \n",
        "  # binary entropy gradients\n",
        "  dw = (1 / m) * np.dot(X, (A - Y).T)  #weight update\n",
        "  db = (1 / m) * np.sum(A - Y)\n",
        "  \n",
        "  grads = {\"dw\": dw,      #storing gradients in dictionary\n",
        "            \"db\": db }\n",
        "    \n",
        "  return L,grads\n",
        "\n",
        " \n",
        "#binary cross entropy   \n",
        "def compute_loss(Y, Y_hat):   \n",
        "\n",
        "  m = Y.shape[1]\n",
        "  L = -(1./m) * ( np.sum( np.multiply(np.log(Y_hat),Y) ) + np.sum( np.multiply(np.log(1-Y_hat),(1-Y)) ) )\n",
        "\n",
        "  return L\n",
        "\n",
        "#binary cross entropy with activation function\n",
        "def binary_cross_entropy(w,b,X,Y):  \n",
        "  \n",
        "  m =  m = X.shape[1]\n",
        "  A = sigmoid(np.dot(w.T, X) + b)\n",
        "  L = (- 1 / m) * np.sum(Y * np.log(A) + (1 - Y) * (np.log(1 - A)))  #calculating loss\n",
        " \n",
        "  L = np.squeeze(L)\n",
        "  dw = (1 / m) * np.dot(X, (A - Y).T)  #weight update\n",
        "  db = (1 / m) * np.sum(A - Y)\n",
        "  \n",
        "  grads = {\"dw\": dw,    #storing gradients in dictionary\n",
        "            \"db\": db }\n",
        "    \n",
        "  return L,grads"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5xpFrQGzvkWE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#We initialize weights and biases\n",
        "def initialize_wb(size):\n",
        "  w = np.zeros(shape=(size, 1))  #an empty vector for weight\n",
        "  b = 0  #bias simply is 0, scalar quantity\n",
        "  return w,b\n",
        "\n",
        "\n",
        "#this function splits the training set into slices to train in batches\n",
        "\n",
        "def mini_batches(X_whole, Y_whole, batchsize):\n",
        "  for index in range(0, X_whole.shape[0] - batchsize + 1, batchsize):\n",
        "    batch = slice(index, index + batchsize)\n",
        "    yield X_whole[batch], Y_whole[batch]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RUrK4qmVwZPE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#Defining some hyperparameters\n",
        "learning_rate = 0.01\n",
        "batch_size = 128\n",
        "classes = 1\n",
        "\n",
        "#our main optimize function - Minibatch - Stochastic Gradient Descent is defined here\n",
        "\n",
        "def SGD(digit,w, b, X, Y, epochs, learning_rate):\n",
        "  \n",
        "  loss_total = []\n",
        "  #print(X.shape)\n",
        "  #print(Y.shape)\n",
        "  \n",
        "  print('\\n Results for Network %i using Binary Cross Entropy Loss\\n\\n' % (digit))\n",
        "  \n",
        "  for i in range(epochs):\n",
        "      \n",
        "    for batch in mini_batches(X.T, Y.T, batch_size):\n",
        "    \n",
        "       x_batch, y_batch = batch\n",
        "       #print(x_batch.shape)\n",
        "       L,grads = binary_cross_entropy(w, b, x_batch.T, y_batch.T)  #Calling function MSE \n",
        "       dw = grads[\"dw\"]\n",
        "       db = grads[\"db\"]\n",
        "       w = w - learning_rate * dw   #updating weights\n",
        "       b = b - learning_rate * db   #updating biases\n",
        "        \n",
        "       #print(w.shape)\n",
        "\n",
        "       \n",
        "    loss_total.append(L)\n",
        "    print (\" Epoch %i , Loss: %f\" % (i, L))\n",
        "            \n",
        "  final_wb = {\"w\": w,\n",
        "            \"b\": b}\n",
        "\n",
        "  grads = {\"dw\": dw,\n",
        "               \"db\": db}\n",
        "\n",
        "  return final_wb, loss_total"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LWeo6GuVwkKk",
        "colab_type": "code",
        "outputId": "0d290512-69fb-4df3-c917-b7bc3e97ebf5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3757
        }
      },
      "cell_type": "code",
      "source": [
        "#This evaluate function is used to test our model\n",
        "\n",
        "def evaluate(w, b, X):\n",
        "  m = X.shape[1]\n",
        "  y_hat = np.zeros((1, m))\n",
        "  w = w.reshape(X.shape[0], classes)\n",
        "  A = sigmoid(np.dot(w.T, X) + b)\n",
        "  for i in range(A.shape[1]):\n",
        "    y_hat[0, i] = 1 if A[0, i] > 0.5 else 0\n",
        "  return y_hat\n",
        "\n",
        "\n",
        "#This function defines our model, test set is also then evaluated\n",
        "\n",
        "def model(digit,X_train, Y_train, X_test, Y_test, num_iterations=2000):\n",
        "   w, b = initialize_wb(X_train.shape[0])\n",
        "   final_wb,loss = SGD(digit,w, b, X_train, Y_train, num_iterations, learning_rate)\n",
        "   #print(w.shape)\n",
        "   w = final_wb[\"w\"]\n",
        "   b = final_wb[\"b\"]\n",
        "   test_accuracy = evaluate(w, b, X_test)\n",
        "   print(\"\\n\\n Test accuracy for digit {} is : {} %\".format(digit,(100 - np.mean(np.abs(test_accuracy - Y_test)) * 100)))\n",
        "\n",
        "\n",
        "\n",
        "# 10 Networks - each with 1 digit as target class (positive), rest digits (negative)\n",
        "# a for loop for training 10 classifiers\n",
        "\n",
        "for digit in range(0,10):\n",
        "  \n",
        "  #modify original labels each time\n",
        "  train_labels,test_labels=modify_labels(digit,train_labels_original,test_labels_original)\n",
        "  #we call our model function and begin training\n",
        "  train_classifier = model(digit,train_images_original, train_labels, test_images_original, test_labels, num_iterations = 15)   \n",
        "  "
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Results for Network 0 using Binary Cross Entropy Loss\n",
            "\n",
            "\n",
            " Epoch 0 , Loss: 0.067639\n",
            " Epoch 1 , Loss: 0.043202\n",
            " Epoch 2 , Loss: 0.034105\n",
            " Epoch 3 , Loss: 0.029139\n",
            " Epoch 4 , Loss: 0.025917\n",
            " Epoch 5 , Loss: 0.023615\n",
            " Epoch 6 , Loss: 0.021869\n",
            " Epoch 7 , Loss: 0.020489\n",
            " Epoch 8 , Loss: 0.019365\n",
            " Epoch 9 , Loss: 0.018428\n",
            " Epoch 10 , Loss: 0.017634\n",
            " Epoch 11 , Loss: 0.016949\n",
            " Epoch 12 , Loss: 0.016353\n",
            " Epoch 13 , Loss: 0.015827\n",
            " Epoch 14 , Loss: 0.015360\n",
            "\n",
            "\n",
            " Test accuracy for digit 0 is : 99.06 %\n",
            "\n",
            " Results for Network 1 using Binary Cross Entropy Loss\n",
            "\n",
            "\n",
            " Epoch 0 , Loss: 0.084486\n",
            " Epoch 1 , Loss: 0.058800\n",
            " Epoch 2 , Loss: 0.048586\n",
            " Epoch 3 , Loss: 0.042851\n",
            " Epoch 4 , Loss: 0.039107\n",
            " Epoch 5 , Loss: 0.036449\n",
            " Epoch 6 , Loss: 0.034458\n",
            " Epoch 7 , Loss: 0.032911\n",
            " Epoch 8 , Loss: 0.031675\n",
            " Epoch 9 , Loss: 0.030668\n",
            " Epoch 10 , Loss: 0.029832\n",
            " Epoch 11 , Loss: 0.029130\n",
            " Epoch 12 , Loss: 0.028533\n",
            " Epoch 13 , Loss: 0.028020\n",
            " Epoch 14 , Loss: 0.027576\n",
            "\n",
            "\n",
            " Test accuracy for digit 1 is : 99.01 %\n",
            "\n",
            " Results for Network 2 using Binary Cross Entropy Loss\n",
            "\n",
            "\n",
            " Epoch 0 , Loss: 0.125962\n",
            " Epoch 1 , Loss: 0.078804\n",
            " Epoch 2 , Loss: 0.060508\n",
            " Epoch 3 , Loss: 0.050790\n",
            " Epoch 4 , Loss: 0.044732\n",
            " Epoch 5 , Loss: 0.040566\n",
            " Epoch 6 , Loss: 0.037503\n",
            " Epoch 7 , Loss: 0.035139\n",
            " Epoch 8 , Loss: 0.033247\n",
            " Epoch 9 , Loss: 0.031688\n",
            " Epoch 10 , Loss: 0.030376\n",
            " Epoch 11 , Loss: 0.029250\n",
            " Epoch 12 , Loss: 0.028269\n",
            " Epoch 13 , Loss: 0.027404\n",
            " Epoch 14 , Loss: 0.026633\n",
            "\n",
            "\n",
            " Test accuracy for digit 2 is : 97.51 %\n",
            "\n",
            " Results for Network 3 using Binary Cross Entropy Loss\n",
            "\n",
            "\n",
            " Epoch 0 , Loss: 0.135836\n",
            " Epoch 1 , Loss: 0.097860\n",
            " Epoch 2 , Loss: 0.081288\n",
            " Epoch 3 , Loss: 0.071819\n",
            " Epoch 4 , Loss: 0.065608\n",
            " Epoch 5 , Loss: 0.061166\n",
            " Epoch 6 , Loss: 0.057792\n",
            " Epoch 7 , Loss: 0.055116\n",
            " Epoch 8 , Loss: 0.052925\n",
            " Epoch 9 , Loss: 0.051086\n",
            " Epoch 10 , Loss: 0.049511\n",
            " Epoch 11 , Loss: 0.048141\n",
            " Epoch 12 , Loss: 0.046934\n",
            " Epoch 13 , Loss: 0.045860\n",
            " Epoch 14 , Loss: 0.044894\n",
            "\n",
            "\n",
            " Test accuracy for digit 3 is : 97.2 %\n",
            "\n",
            " Results for Network 4 using Binary Cross Entropy Loss\n",
            "\n",
            "\n",
            " Epoch 0 , Loss: 0.110093\n",
            " Epoch 1 , Loss: 0.068097\n",
            " Epoch 2 , Loss: 0.050452\n",
            " Epoch 3 , Loss: 0.040790\n",
            " Epoch 4 , Loss: 0.034650\n",
            " Epoch 5 , Loss: 0.030362\n",
            " Epoch 6 , Loss: 0.027171\n",
            " Epoch 7 , Loss: 0.024688\n",
            " Epoch 8 , Loss: 0.022691\n",
            " Epoch 9 , Loss: 0.021043\n",
            " Epoch 10 , Loss: 0.019657\n",
            " Epoch 11 , Loss: 0.018472\n",
            " Epoch 12 , Loss: 0.017446\n",
            " Epoch 13 , Loss: 0.016548\n",
            " Epoch 14 , Loss: 0.015753\n",
            "\n",
            "\n",
            " Test accuracy for digit 4 is : 97.64 %\n",
            "\n",
            " Results for Network 5 using Binary Cross Entropy Loss\n",
            "\n",
            "\n",
            " Epoch 0 , Loss: 0.141927\n",
            " Epoch 1 , Loss: 0.099593\n",
            " Epoch 2 , Loss: 0.081078\n",
            " Epoch 3 , Loss: 0.070913\n",
            " Epoch 4 , Loss: 0.064484\n",
            " Epoch 5 , Loss: 0.060027\n",
            " Epoch 6 , Loss: 0.056738\n",
            " Epoch 7 , Loss: 0.054203\n",
            " Epoch 8 , Loss: 0.052182\n",
            " Epoch 9 , Loss: 0.050532\n",
            " Epoch 10 , Loss: 0.049158\n",
            " Epoch 11 , Loss: 0.047995\n",
            " Epoch 12 , Loss: 0.046998\n",
            " Epoch 13 , Loss: 0.046134\n",
            " Epoch 14 , Loss: 0.045377\n",
            "\n",
            "\n",
            " Test accuracy for digit 5 is : 96.46 %\n",
            "\n",
            " Results for Network 6 using Binary Cross Entropy Loss\n",
            "\n",
            "\n",
            " Epoch 0 , Loss: 0.085477\n",
            " Epoch 1 , Loss: 0.049985\n",
            " Epoch 2 , Loss: 0.037093\n",
            " Epoch 3 , Loss: 0.030299\n",
            " Epoch 4 , Loss: 0.026072\n",
            " Epoch 5 , Loss: 0.023186\n",
            " Epoch 6 , Loss: 0.021095\n",
            " Epoch 7 , Loss: 0.019516\n",
            " Epoch 8 , Loss: 0.018288\n",
            " Epoch 9 , Loss: 0.017310\n",
            " Epoch 10 , Loss: 0.016518\n",
            " Epoch 11 , Loss: 0.015866\n",
            " Epoch 12 , Loss: 0.015324\n",
            " Epoch 13 , Loss: 0.014869\n",
            " Epoch 14 , Loss: 0.014483\n",
            "\n",
            "\n",
            " Test accuracy for digit 6 is : 98.18 %\n",
            "\n",
            " Results for Network 7 using Binary Cross Entropy Loss\n",
            "\n",
            "\n",
            " Epoch 0 , Loss: 0.103781\n",
            " Epoch 1 , Loss: 0.068119\n",
            " Epoch 2 , Loss: 0.053419\n",
            " Epoch 3 , Loss: 0.045181\n",
            " Epoch 4 , Loss: 0.039833\n",
            " Epoch 5 , Loss: 0.036049\n",
            " Epoch 6 , Loss: 0.033214\n",
            " Epoch 7 , Loss: 0.031003\n",
            " Epoch 8 , Loss: 0.029227\n",
            " Epoch 9 , Loss: 0.027767\n",
            " Epoch 10 , Loss: 0.026544\n",
            " Epoch 11 , Loss: 0.025505\n",
            " Epoch 12 , Loss: 0.024610\n",
            " Epoch 13 , Loss: 0.023832\n",
            " Epoch 14 , Loss: 0.023148\n",
            "\n",
            "\n",
            " Test accuracy for digit 7 is : 98.18 %\n",
            "\n",
            " Results for Network 8 using Binary Cross Entropy Loss\n",
            "\n",
            "\n",
            " Epoch 0 , Loss: 0.198947\n",
            " Epoch 1 , Loss: 0.160095\n",
            " Epoch 2 , Loss: 0.141479\n",
            " Epoch 3 , Loss: 0.130443\n",
            " Epoch 4 , Loss: 0.123069\n",
            " Epoch 5 , Loss: 0.117746\n",
            " Epoch 6 , Loss: 0.113682\n",
            " Epoch 7 , Loss: 0.110447\n",
            " Epoch 8 , Loss: 0.107785\n",
            " Epoch 9 , Loss: 0.105533\n",
            " Epoch 10 , Loss: 0.103587\n",
            " Epoch 11 , Loss: 0.101874\n",
            " Epoch 12 , Loss: 0.100344\n",
            " Epoch 13 , Loss: 0.098959\n",
            " Epoch 14 , Loss: 0.097693\n",
            "\n",
            "\n",
            " Test accuracy for digit 8 is : 94.78 %\n",
            "\n",
            " Results for Network 9 using Binary Cross Entropy Loss\n",
            "\n",
            "\n",
            " Epoch 0 , Loss: 0.214862\n",
            " Epoch 1 , Loss: 0.179450\n",
            " Epoch 2 , Loss: 0.160577\n",
            " Epoch 3 , Loss: 0.148441\n",
            " Epoch 4 , Loss: 0.139883\n",
            " Epoch 5 , Loss: 0.133483\n",
            " Epoch 6 , Loss: 0.128493\n",
            " Epoch 7 , Loss: 0.124479\n",
            " Epoch 8 , Loss: 0.121168\n",
            " Epoch 9 , Loss: 0.118383\n",
            " Epoch 10 , Loss: 0.116001\n",
            " Epoch 11 , Loss: 0.113935\n",
            " Epoch 12 , Loss: 0.112121\n",
            " Epoch 13 , Loss: 0.110512\n",
            " Epoch 14 , Loss: 0.109072\n",
            "\n",
            "\n",
            " Test accuracy for digit 9 is : 95.8 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}