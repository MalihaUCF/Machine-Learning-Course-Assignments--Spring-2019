{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "k-fold_CNN-5.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MalihaUCF/Machine-Learning-Course-Assignments-Spring-2019/blob/master/Assignment%202/k_fold_CNN_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "amevUdJWUBwC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        },
        "outputId": "dbba4fc4-3c86-413c-c62f-761ea5eff5bf"
      },
      "cell_type": "code",
      "source": [
        "#Loading CIFAR-10 dataset\n",
        "\n",
        "from keras.datasets import cifar10\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import keras\n",
        "\n",
        "\n",
        "#creating train,validate and test sets\n",
        "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
        "train_images1, val_images1, train_labels1, val_labels1 = train_test_split(train_images,train_labels,test_size=0.0002,stratify=train_labels)\n",
        "\n",
        "print(train_images.shape)\n",
        "print(train_images1.shape)\n",
        "print(val_images1.shape)\n",
        "print(test_images.shape)\n",
        "print(train_labels1.shape)\n",
        "\n",
        "\n",
        "for i in range(0,50):\n",
        "  print(train_labels[i])\n",
        "\n",
        "def k_fold_dataset(i,train_images1,train_labels1):\n",
        "  val_index_a=i*10000\n",
        "  val_index_b=val_index_a+10000\n",
        "  \n",
        "  #splitting train images into train and val set\n",
        "  #val set\n",
        "  X_val=train_images1[val_index_a:val_index_b]\n",
        "  y_val=train_labels1[val_index_a:val_index_b]\n",
        "  \n",
        "  #train set\n",
        "  X_train=np.append(train_images1[0:val_index_a],train_images1[val_index_b:50000],axis=0)\n",
        "  y_train=np.append(train_labels1[0:val_index_a],train_labels1[val_index_b:50000],axis=0)\n",
        "  \n",
        "  \n",
        "  return X_train,y_train,X_val,y_val\n",
        "  \n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 32, 32, 3)\n",
            "(49990, 32, 32, 3)\n",
            "(10, 32, 32, 3)\n",
            "(10000, 32, 32, 3)\n",
            "(49990, 1)\n",
            "[6]\n",
            "[9]\n",
            "[9]\n",
            "[4]\n",
            "[1]\n",
            "[1]\n",
            "[2]\n",
            "[7]\n",
            "[8]\n",
            "[3]\n",
            "[4]\n",
            "[7]\n",
            "[7]\n",
            "[2]\n",
            "[9]\n",
            "[9]\n",
            "[9]\n",
            "[3]\n",
            "[2]\n",
            "[6]\n",
            "[4]\n",
            "[3]\n",
            "[6]\n",
            "[6]\n",
            "[2]\n",
            "[6]\n",
            "[3]\n",
            "[5]\n",
            "[4]\n",
            "[0]\n",
            "[0]\n",
            "[9]\n",
            "[1]\n",
            "[3]\n",
            "[4]\n",
            "[0]\n",
            "[3]\n",
            "[7]\n",
            "[3]\n",
            "[3]\n",
            "[5]\n",
            "[2]\n",
            "[2]\n",
            "[7]\n",
            "[1]\n",
            "[1]\n",
            "[1]\n",
            "[2]\n",
            "[2]\n",
            "[0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IpagRuAcV3Xs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_train_val(n,data,labels):#k=5\n",
        "    val_start=n*10000\n",
        "    val_end=val_start+10000\n",
        "    x_val=data[val_start:val_end]\n",
        "    y_val=labels[val_start:val_end]\n",
        "    x_train=np.append(data[0:val_start],data[val_end:50000],axis=0)\n",
        "    y_train=np.append(labels[0:val_start],labels[val_end:50000],axis=0)\n",
        "    y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "    y_val = keras.utils.to_categorical(y_val, num_classes)\n",
        "\n",
        "    x_train = x_train.astype('float32')\n",
        "    x_val=x_val.astype('float32')\n",
        "    return x_train,y_train,x_val,y_val"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZOYmGTpPbnlm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i in range(0,5):\n",
        "    x_train,y_train,x_val,y_val=get_train_val(i,x_main, y_main)\n",
        "    train_model(i,x_train,y_train,x_val,y_val,x_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jFAElkl0TBiV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "epochs = 30\n",
        "data_augmentation = False\n",
        "num_predictions = 20\n",
        "\n",
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "\n",
        "\n",
        "#using our best model with k-fold\n",
        "def train_model1(i,X_train,y_train,X_val,y_val,test_images,test_labels):\n",
        "  \n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(32, (3, 3), padding='same',\n",
        "                   input_shape=X_train.shape[1:]))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Conv2D(32, (3, 3)))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "\n",
        "  model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Conv2D(64, (3, 3)))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Dropout(0.25))\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(512))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(num_classes))\n",
        "  model.add(Activation('softmax'))\n",
        "  \n",
        " \n",
        "\n",
        "  #using ADAM\n",
        "\n",
        "  Adam=keras.optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "\n",
        "\n",
        "  # Let's train our best model using Adam\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "                optimizer=Adam,\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "  if not data_augmentation:\n",
        "      print('Not using data augmentation.')\n",
        "      history=model.fit(X_train, y_train,\n",
        "                batch_size=batch_size,\n",
        "                epochs=epochs,\n",
        "                validation_data=(X_val, y_val),\n",
        "                shuffle=True)\n",
        "  else:\n",
        "      print('Using real-time data augmentation.')\n",
        "      # This will do preprocessing and realtime data augmentation:\n",
        "      datagen = ImageDataGenerator(\n",
        "          featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "          samplewise_center=False,  # set each sample mean to 0\n",
        "          featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "          samplewise_std_normalization=False,  # divide each input by its std\n",
        "          zca_whitening=False,  # apply ZCA whitening\n",
        "          zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
        "          rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "          # randomly shift images horizontally (fraction of total width)\n",
        "          width_shift_range=0.1,\n",
        "          # randomly shift images vertically (fraction of total height)\n",
        "          height_shift_range=0.1,\n",
        "          shear_range=0.,  # set range for random shear\n",
        "          zoom_range=0.,  # set range for random zoom\n",
        "          channel_shift_range=0.,  # set range for random channel shifts\n",
        "          # set mode for filling points outside the input boundaries\n",
        "          fill_mode='nearest',\n",
        "          cval=0.,  # value used for fill_mode = \"constant\"\n",
        "          horizontal_flip=True,  # randomly flip images\n",
        "          vertical_flip=False,  # randomly flip images\n",
        "          # set rescaling factor (applied before any other transformation)\n",
        "          rescale=None,\n",
        "          # set function that will be applied on each input\n",
        "          preprocessing_function=None,\n",
        "          # image data format, either \"channels_first\" or \"channels_last\"\n",
        "          data_format=None,\n",
        "          # fraction of images reserved for validation (strictly between 0 and 1)\n",
        "          validation_split=0.0)\n",
        "\n",
        "      # Compute quantities required for feature-wise normalization\n",
        "      # (std, mean, and principal components if ZCA whitening is applied).\n",
        "      datagen.fit(X_train)\n",
        "\n",
        "      # Fit the model on the batches generated by datagen.flow().\n",
        "      model.fit_generator(datagen.flow(X_train, y_train,\n",
        "                                       batch_size=batch_size),\n",
        "                          epochs=epochs,steps_per_epoch=100,validation_steps=50,\n",
        "                          validation_data=(X_val, y_val))\n",
        "\n",
        "  \n",
        "  \n",
        "  from sklearn.metrics import classification_report, confusion_matrix\n",
        "  import os\n",
        "  from keras.models import load_model\n",
        "\n",
        "\n",
        "  y_pred=model.predict(test_images) \n",
        "  y_pred = [np.argmax(entry) for entry in y_pred]\n",
        "  y_true = [np.argmax(entry) for entry in test_labels]\n",
        "\n",
        "  print('Confusion Matrix\\n\\n')\n",
        "  print(confusion_matrix(y_true, y_pred))  \n",
        "\n",
        "  print(classification_report(y_pred,y_true))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oW-y8ZLCkoJ-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2465
        },
        "outputId": "df748ab3-0fd0-4d4e-d084-82ed1bb847f9"
      },
      "cell_type": "code",
      "source": [
        "from random import shuffle\n",
        "\n",
        "num_classes=10\n",
        "# Convert class vectors to binary class matrices.\n",
        "test_labels = keras.utils.to_categorical(test_labels, num_classes)\n",
        "\n",
        "#train_images=np.array(train_images)\n",
        "#shuffle(train_images)\n",
        "\n",
        "for a in range(0,5):\n",
        "    X_train,y_train,X_val,y_val=k_fold_dataset(a,train_images1,train_labels1)\n",
        "    \n",
        "    X_train = X_train.astype('float32')\n",
        "    X_val = X_val.astype('float32')\n",
        "    X_train /= 255\n",
        "    X_val /= 255\n",
        "    \n",
        "    \n",
        "    y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "    y_val=keras.utils.to_categorical(y_val, num_classes)\n",
        "    \n",
        "    print(train_images1.shape)\n",
        "    print(X_train.shape)\n",
        "    print(X_val.shape)\n",
        "    print(test_images.shape)  \n",
        "    print(y_val.shape)\n",
        "    \n",
        "    train_model1(a,X_train,y_train,X_val,y_val,test_images, test_labels)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(49990, 32, 32, 3)\n",
            "(39990, 32, 32, 3)\n",
            "(10000, 32, 32, 3)\n",
            "(10000, 32, 32, 3)\n",
            "(10000, 10)\n",
            "Not using data augmentation.\n",
            "Train on 39990 samples, validate on 10000 samples\n",
            "Epoch 1/30\n",
            "39990/39990 [==============================] - 512s 13ms/step - loss: 1.8280 - acc: 0.3276 - val_loss: 1.6210 - val_acc: 0.4159\n",
            "Epoch 2/30\n",
            "39990/39990 [==============================] - 512s 13ms/step - loss: 1.4977 - acc: 0.4568 - val_loss: 1.3925 - val_acc: 0.5051\n",
            "Epoch 3/30\n",
            "39990/39990 [==============================] - 516s 13ms/step - loss: 1.3776 - acc: 0.5026 - val_loss: 1.2736 - val_acc: 0.5450\n",
            "Epoch 4/30\n",
            "39990/39990 [==============================] - 512s 13ms/step - loss: 1.2871 - acc: 0.5379 - val_loss: 1.1936 - val_acc: 0.5747\n",
            "Epoch 5/30\n",
            "39990/39990 [==============================] - 509s 13ms/step - loss: 1.2093 - acc: 0.5701 - val_loss: 1.1202 - val_acc: 0.6017\n",
            "Epoch 6/30\n",
            "39990/39990 [==============================] - 511s 13ms/step - loss: 1.1519 - acc: 0.5937 - val_loss: 1.0940 - val_acc: 0.6083\n",
            "Epoch 7/30\n",
            "39990/39990 [==============================] - 497s 12ms/step - loss: 1.0979 - acc: 0.6135 - val_loss: 1.0184 - val_acc: 0.6449\n",
            "Epoch 8/30\n",
            "39990/39990 [==============================] - 498s 12ms/step - loss: 1.0454 - acc: 0.6313 - val_loss: 0.9982 - val_acc: 0.6441\n",
            "Epoch 9/30\n",
            "39990/39990 [==============================] - 503s 13ms/step - loss: 0.9979 - acc: 0.6500 - val_loss: 0.9351 - val_acc: 0.6683\n",
            "Epoch 10/30\n",
            "39990/39990 [==============================] - 503s 13ms/step - loss: 0.9556 - acc: 0.6633 - val_loss: 0.9039 - val_acc: 0.6778\n",
            "Epoch 11/30\n",
            "39990/39990 [==============================] - 506s 13ms/step - loss: 0.9205 - acc: 0.6760 - val_loss: 0.8911 - val_acc: 0.6814\n",
            "Epoch 12/30\n",
            "39990/39990 [==============================] - 507s 13ms/step - loss: 0.8846 - acc: 0.6912 - val_loss: 0.8727 - val_acc: 0.6910\n",
            "Epoch 13/30\n",
            "39990/39990 [==============================] - 504s 13ms/step - loss: 0.8530 - acc: 0.7010 - val_loss: 0.8241 - val_acc: 0.7105\n",
            "Epoch 14/30\n",
            "39990/39990 [==============================] - 506s 13ms/step - loss: 0.8269 - acc: 0.7120 - val_loss: 0.8103 - val_acc: 0.7156\n",
            "Epoch 15/30\n",
            "39990/39990 [==============================] - 513s 13ms/step - loss: 0.7945 - acc: 0.7219 - val_loss: 0.8006 - val_acc: 0.7125\n",
            "Epoch 16/30\n",
            "39990/39990 [==============================] - 507s 13ms/step - loss: 0.7754 - acc: 0.7299 - val_loss: 0.8161 - val_acc: 0.7164\n",
            "Epoch 17/30\n",
            "39990/39990 [==============================] - 506s 13ms/step - loss: 0.7468 - acc: 0.7372 - val_loss: 0.7503 - val_acc: 0.7374\n",
            "Epoch 18/30\n",
            "39990/39990 [==============================] - 501s 13ms/step - loss: 0.7243 - acc: 0.7469 - val_loss: 0.7332 - val_acc: 0.7417\n",
            "Epoch 19/30\n",
            "39990/39990 [==============================] - 506s 13ms/step - loss: 0.6997 - acc: 0.7566 - val_loss: 0.7292 - val_acc: 0.7410\n",
            "Epoch 20/30\n",
            "39990/39990 [==============================] - 505s 13ms/step - loss: 0.6815 - acc: 0.7621 - val_loss: 0.7225 - val_acc: 0.7423\n",
            "Epoch 21/30\n",
            "39990/39990 [==============================] - 503s 13ms/step - loss: 0.6613 - acc: 0.7690 - val_loss: 0.7058 - val_acc: 0.7501\n",
            "Epoch 22/30\n",
            "39990/39990 [==============================] - 503s 13ms/step - loss: 0.6401 - acc: 0.7778 - val_loss: 0.7065 - val_acc: 0.7499\n",
            "Epoch 23/30\n",
            "39990/39990 [==============================] - 505s 13ms/step - loss: 0.6211 - acc: 0.7838 - val_loss: 0.6908 - val_acc: 0.7584\n",
            "Epoch 24/30\n",
            "39990/39990 [==============================] - 512s 13ms/step - loss: 0.6060 - acc: 0.7858 - val_loss: 0.6992 - val_acc: 0.7538\n",
            "Epoch 25/30\n",
            "39990/39990 [==============================] - 511s 13ms/step - loss: 0.5914 - acc: 0.7924 - val_loss: 0.6789 - val_acc: 0.7604\n",
            "Epoch 26/30\n",
            "39990/39990 [==============================] - 506s 13ms/step - loss: 0.5749 - acc: 0.7988 - val_loss: 0.6764 - val_acc: 0.7613\n",
            "Epoch 27/30\n",
            "39990/39990 [==============================] - 510s 13ms/step - loss: 0.5583 - acc: 0.8025 - val_loss: 0.6647 - val_acc: 0.7665\n",
            "Epoch 28/30\n",
            "39990/39990 [==============================] - 509s 13ms/step - loss: 0.5452 - acc: 0.8101 - val_loss: 0.6689 - val_acc: 0.7685\n",
            "Epoch 29/30\n",
            "39990/39990 [==============================] - 514s 13ms/step - loss: 0.5247 - acc: 0.8153 - val_loss: 0.6889 - val_acc: 0.7572\n",
            "Epoch 30/30\n",
            "39990/39990 [==============================] - 505s 13ms/step - loss: 0.5165 - acc: 0.8193 - val_loss: 0.6496 - val_acc: 0.7737\n",
            "Confusion Matrix\n",
            "\n",
            "\n",
            "[[907  14   2   1   3   7   0   6  43  17]\n",
            " [ 53 832   0   1   1   2   0   2  56  53]\n",
            " [293  21 258  48  27 151   8  98  54  42]\n",
            " [143  30  11 269  14 264  10  73  90  96]\n",
            " [194  10  24  42 282 139  11 211  52  35]\n",
            " [ 66  10   9  54   4 686   3  88  28  52]\n",
            " [103  28  12  62  12 128 406  58 119  72]\n",
            " [ 81   3   1   3   6  47   0 802   8  49]\n",
            " [189  11   0   1   0   3   0   1 777  18]\n",
            " [ 94  68   1   2   0   3   0   7  23 802]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.43      0.58      2123\n",
            "           1       0.83      0.81      0.82      1027\n",
            "           2       0.26      0.81      0.39       318\n",
            "           3       0.27      0.56      0.36       483\n",
            "           4       0.28      0.81      0.42       349\n",
            "           5       0.69      0.48      0.56      1430\n",
            "           6       0.41      0.93      0.56       438\n",
            "           7       0.80      0.60      0.68      1346\n",
            "           8       0.78      0.62      0.69      1250\n",
            "           9       0.80      0.65      0.72      1236\n",
            "\n",
            "   micro avg       0.60      0.60      0.60     10000\n",
            "   macro avg       0.60      0.67      0.58     10000\n",
            "weighted avg       0.73      0.60      0.62     10000\n",
            "\n",
            "(49990, 32, 32, 3)\n",
            "(39990, 32, 32, 3)\n",
            "(10000, 32, 32, 3)\n",
            "(10000, 32, 32, 3)\n",
            "(10000, 10)\n",
            "Not using data augmentation.\n",
            "Train on 39990 samples, validate on 10000 samples\n",
            "Epoch 1/30\n",
            "39990/39990 [==============================] - 509s 13ms/step - loss: 1.7841 - acc: 0.3436 - val_loss: 1.4845 - val_acc: 0.4553\n",
            "Epoch 2/30\n",
            "39990/39990 [==============================] - 505s 13ms/step - loss: 1.4557 - acc: 0.4711 - val_loss: 1.3235 - val_acc: 0.5282\n",
            "Epoch 3/30\n",
            "39990/39990 [==============================] - 507s 13ms/step - loss: 1.3390 - acc: 0.5185 - val_loss: 1.2484 - val_acc: 0.5551\n",
            "Epoch 4/30\n",
            "39990/39990 [==============================] - 509s 13ms/step - loss: 1.2569 - acc: 0.5495 - val_loss: 1.1853 - val_acc: 0.5880\n",
            "Epoch 5/30\n",
            "39990/39990 [==============================] - 509s 13ms/step - loss: 1.1960 - acc: 0.5728 - val_loss: 1.1296 - val_acc: 0.5988\n",
            "Epoch 6/30\n",
            "39990/39990 [==============================] - 507s 13ms/step - loss: 1.1378 - acc: 0.6023 - val_loss: 1.0671 - val_acc: 0.6236\n",
            "Epoch 7/30\n",
            "39990/39990 [==============================] - 509s 13ms/step - loss: 1.0835 - acc: 0.6168 - val_loss: 1.0270 - val_acc: 0.6337\n",
            "Epoch 8/30\n",
            "39990/39990 [==============================] - 518s 13ms/step - loss: 1.0396 - acc: 0.6342 - val_loss: 0.9886 - val_acc: 0.6558\n",
            "Epoch 9/30\n",
            "39990/39990 [==============================] - 522s 13ms/step - loss: 0.9953 - acc: 0.6483 - val_loss: 0.9709 - val_acc: 0.6647\n",
            "Epoch 10/30\n",
            "39990/39990 [==============================] - 518s 13ms/step - loss: 0.9571 - acc: 0.6650 - val_loss: 0.9321 - val_acc: 0.6729\n",
            "Epoch 11/30\n",
            "39990/39990 [==============================] - 520s 13ms/step - loss: 0.9295 - acc: 0.6715 - val_loss: 0.9059 - val_acc: 0.6846\n",
            "Epoch 12/30\n",
            "39990/39990 [==============================] - 514s 13ms/step - loss: 0.8955 - acc: 0.6879 - val_loss: 0.8788 - val_acc: 0.6981\n",
            "Epoch 13/30\n",
            "39990/39990 [==============================] - 516s 13ms/step - loss: 0.8638 - acc: 0.6970 - val_loss: 0.8614 - val_acc: 0.6948\n",
            "Epoch 14/30\n",
            "39990/39990 [==============================] - 520s 13ms/step - loss: 0.8375 - acc: 0.7052 - val_loss: 0.8444 - val_acc: 0.7083\n",
            "Epoch 15/30\n",
            "39990/39990 [==============================] - 515s 13ms/step - loss: 0.8140 - acc: 0.7144 - val_loss: 0.8405 - val_acc: 0.7096\n",
            "Epoch 16/30\n",
            "39990/39990 [==============================] - 526s 13ms/step - loss: 0.7901 - acc: 0.7225 - val_loss: 0.8197 - val_acc: 0.7159\n",
            "Epoch 17/30\n",
            "39990/39990 [==============================] - 515s 13ms/step - loss: 0.7674 - acc: 0.7305 - val_loss: 0.8108 - val_acc: 0.7185\n",
            "Epoch 18/30\n",
            "39990/39990 [==============================] - 512s 13ms/step - loss: 0.7458 - acc: 0.7385 - val_loss: 0.7869 - val_acc: 0.7245\n",
            "Epoch 19/30\n",
            "39990/39990 [==============================] - 507s 13ms/step - loss: 0.7249 - acc: 0.7465 - val_loss: 0.7891 - val_acc: 0.7221\n",
            "Epoch 20/30\n",
            " 6912/39990 [====>.........................] - ETA: 6:32 - loss: 0.7012 - acc: 0.7525"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RIxfqpwrtIuW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Saving the model as an h5 file\n",
        "\n",
        "good_model='adam_dropout_hold_out.h5'\n",
        "model.save(good_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "72cyo6cntUGx",
        "colab_type": "code",
        "outputId": "662d704f-2f50-4b8c-de9f-5ecafe5a03a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "adam_dropout_hold_out.h5  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BDZ1t2OBtUwh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "with open(good_model, 'r') as f:\n",
        "  files.download(good_model)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}